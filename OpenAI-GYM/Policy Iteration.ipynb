{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "goal_steps = 500\n",
    "score_requirement = 60\n",
    "initial_games = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Episodes (multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateExamples(model = None):\n",
    "    # [OBS, MOVES, SCORE]\n",
    "    allExamples = np.array([])\n",
    "    accepted_scores = np.array([])    # just the scores that met our threshold\n",
    "    \n",
    "    # --------------- ITERATE THROUGH 10000 EPISODE ------------------\n",
    "    for _ in range(initial_games):\n",
    "        # reset env to play again\n",
    "        env.reset();    score = 0\n",
    "        exampleGame = np.reshape( np.array([0, 0, 0, 0, 0, 0]), (1, 6)  )    # [obvservations, action, score]\n",
    "        prev_observation = np.array([0, 0, 0, 0]) # list of 4 elements (our initial observation is all zeros)\n",
    "              \n",
    "        # --------- ITERATE UP TO 500 STEPS PER EPISODE -------------\n",
    "        for _ in range(goal_steps):\n",
    "                       \n",
    "            # --------- GENERATE ACTION ------------\n",
    "            # We can generate random actions or actions from the previous policy (i.e. prev nnet)\n",
    "            if model == None or len(exampleGame) == 0:\n",
    "                action = env.action_space.sample()   # choose random action (0-left or 1-right)\n",
    "            else:\n",
    "                x = torch.tensor(   prev_observation,   dtype = torch.float    )\n",
    "                action_prob, e_score = model.forward(x)\n",
    "                action = np.argmax(   action_prob.detach().numpy()   )                \n",
    "                \n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            \n",
    "            # --------- STORE STATE-ACTION PAIR + SCORE-(need to convert this to E[return]) ------------\n",
    "            # this is the previous ovservation and the action taken from it\n",
    "            t = np.append( prev_observation[0:4], [action, score] ) #creates (6,1) numpy array [obs, act, score]  \n",
    "            #print(np.reshape(t, (1, 6)))\n",
    "            if exampleGame == []:\n",
    "            exampleGame = np.vstack( (exampleGame, np.reshape(t, (1, 6)))  )         \n",
    "                \n",
    "            prev_observation = np.array(observation)\n",
    "            \n",
    "            score += reward    # +1 for every frame we haven't fallen\n",
    "            if done: \n",
    "                break\n",
    "\n",
    "        print(exampleGame.shape)        \n",
    "        print(exampleGame)        \n",
    "                \n",
    "                \n",
    "        # --------- SAVE EXAMPLE (EPISODE) IF (SCORE > THRESHOLD) ----------\n",
    "        # This saves state action pairs in a np.array(np[4x1 state array], np[1x1 action])\n",
    "        # Note, it does not save the score! Therefore all episodes with score > threshold\n",
    "        # are treated equally (not the best way of doing this!)\n",
    "        #if score >= score_requirement:\n",
    "        #    exampleGame = np.append(exampleGame[0:2], score - exampleGame[2])\n",
    "        #    allExamples = np.append(   allExamples, exampleGame   )\n",
    "\n",
    "            #accepted_scores.append(score)\n",
    "            #a, b = np.array(exampleGame)[:, 0:2], np.array(score) - np.array(exampleGame)[:, 2]\n",
    "            #allExamples.append(np.hstack(  (a, np.reshape(b, (len(b), 1)))  )) # shape (72,) != (72, 1)\n",
    "            \n",
    "            \n",
    "    # just in case you wanted to reference later\n",
    "    #training_data_save = np.array(examples)\n",
    "    #np.save('saved.npy',training_data_save)\n",
    "    avg_mean, avg_median = statistics.mean(accepted_scores), statistics.median(accepted_scores)\n",
    "    \n",
    "    # some stats here, to further illustrate the neural network magic!\n",
    "    print('Average accepted score: ', avg_mean)\n",
    "    print('Median score for accepted scores: ', avg_median)\n",
    "    print(Counter(accepted_scores))\n",
    "    \n",
    "    #print(allExamples[0])\n",
    "    # Examples are of the form:\n",
    "    #[array([ [array([0, 0, 0, 0]), 1, 84.0],\n",
    "    #         [array([ 0.01912502,  0.19371378,  0.04909047, -0.24309246]), 0, 83.0],\n",
    "    #         [array([ 0.02299929, -0.00207375,  0.04422862,  0.06466183]), 1, 82.0],\n",
    "    \n",
    "    # allExamples[0] =\n",
    "    # [[array([0, 0, 0, 0]) 1 64.0]\n",
    "    # [array([-0.04165018,  0.18181016, -0.03935655, -0.33425807]) 0 63.0]\n",
    "    # [array([-0.03801397, -0.01273019, -0.04604171, -0.05424134]) 0 62.0]\n",
    "    \n",
    "    return allExamples[0], avg_mean, avg_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, mean, median = GenerateExamples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Policy (Neural Net model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(4, 128)\n",
    "        self.l2 = nn.Linear(128, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.l4 = nn.Linear(128, 32)\n",
    "        \n",
    "        self.dp = nn.Dropout(p = 0.3)  # Suragnair used 0.3\n",
    "        self.fc1 = nn.Linear(32, 2)    # want an action vector output: [log(prob right), log(prob left)]\n",
    "        self.fc2 = nn.Linear(32, 1)    # Output the expected return\n",
    "\n",
    "    def forward(self, obs):\n",
    "        #in_size = x.size(0)\n",
    "        x = F.relu(self.dp(self.l1(obs)))\n",
    "        x = F.relu(self.dp(self.l2(x)))\n",
    "        x = F.relu(self.dp(self.l3(x)))\n",
    "        x = F.relu(self.dp(self.l4(x)))\n",
    "        \n",
    "        #x = x.view(in_size, -1)  # flatten the tensor\n",
    "        a = self.fc1(self.dp(x))\n",
    "        action_probs = F.log_softmax(a, dim = -1)    # choose the dimension such that we get something like \n",
    "                                          # [exp(-0.6723) +  exp(-0.7144)] = 1 for the output\n",
    "        v = self.fc2(self.dp(x))  # get a linear value for the expected return\n",
    "        return action_probs, v                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # initialise the neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model (Policy Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, examples):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    action_loss, value_loss, accuracy = [], [], []\n",
    "    \n",
    "    # ------------- CONVERT TO CORRECT DATA TYPE ----------------\n",
    "    gpu = torch.device(\"cpu\")\n",
    "    print(examples[0])\n",
    "    print(examples[:, 0])\n",
    "    \n",
    "    #allStates= torch.tensor(  np.array([i[0] for i in examples]),  dtype = torch.float, device = gpu)       #reshapes into a (23002, 4) array\n",
    "    #allActions = torch.tensor(  np.array([i[1] for i in examples]), dtype = torch.long, device = gpu)    #reshapes into a (23002, 2) array \n",
    "    #allReturns = torch.tensor(  np.array([i[2] for i in examples]),  dtype = torch.float, device = gpu) \n",
    "    \n",
    "    # We should permute data before batching really. (X is a torch Variable)\n",
    "    #permutation = torch.randperm(X.size()[0])\n",
    "\n",
    "    \n",
    "    for index in range(0, allStates.size()[0], batch_size):        \n",
    "\n",
    "        # -------- GET BATCHES -----------\n",
    "        #indices = permutation[i:i+batch_size]\n",
    "        batch_idx = int(index / batch_size) + 1 #add one so stats print properly\n",
    "        batch_states = allStates[index : index+batch_size] # torch.Size([64, 4])\n",
    "        batch_actions = allActions[index : index+batch_size] # torch.Size([64])\n",
    "        batch_returns = allReturns[index: index+batch_size] # torch.Size([64])\n",
    "\n",
    "        # --------- TRAIN & BACKPROP ----------\n",
    "        optimizer.zero_grad()\n",
    "        pred_actions, state_value = model(batch_states) # torch.Size([64, 2]) and torch.Size([64, 1])\n",
    "        \n",
    "        a_loss = F.nll_loss(pred_actions, batch_actions)\n",
    "        # Suragnair uses tanh for state_values, but their values are E[win] = [-1, 1] where -1 = loss\n",
    "        # Here we are using the length of time that we have been \"up\"\n",
    "        v_loss = F.binary_cross_entropy(torch.sigmoid(state_value[:, 0]), torch.sigmoid(batch_returns))\n",
    "        \n",
    "        action_loss.append(a_loss)\n",
    "        value_loss.append(v_loss)\n",
    "        tot_loss = a_loss + v_loss\n",
    "        \n",
    "        # Should try and get this to work with one hot!\n",
    "        tot_loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        # --------- PRINT STATS --------------\n",
    "        # Get array of predicted actions and compare with target actions to compute accuracy\n",
    "        arg = torch.argmax(pred_actions, dim = 1)\n",
    "        accuracy.append(  1 - (torch.abs(arg - batch_actions).sum().detach().numpy()) / batch_size    ) #counts the different ones\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tAccuracy: {:.5f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * batch_size, \n",
    "                allStates.size()[0],\n",
    "                100 * batch_idx * batch_size / allStates.size()[0], \n",
    "                tot_loss,\n",
    "                accuracy[batch_idx - 1])\n",
    "                  \n",
    "             )\n",
    "\n",
    "    return model, action_loss, value_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, mean, median = GenerateExamples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_net, a_loss, v_loss, batch_acc = train_model(1, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyIteration():\n",
    "    nnet = Net()\n",
    "    \n",
    "    for i in range(10):\n",
    "        training_data, mean, median = GenerateExamples(nnet)\n",
    "        new_nnet, a_loss, v_loss, batch_acc = train_model(1, training_data)\n",
    "        new_data, new_mean, new_median = GenerateExamples(new_nnet)\n",
    "        \n",
    "        if new_mean > mean and new_median > median:\n",
    "            nnet = new_nnet\n",
    "            print(\"policy updated!\")\n",
    "        \n",
    "    return nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = policyIteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
