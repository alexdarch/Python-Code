{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nA pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \\nThe system is controlled by applying a force of +1 or -1 to the cart. \\nThe pendulum starts upright, and the goal is to prevent it from falling over. \\nA reward of +1 is provided for every timestep that the pole remains upright.\\nThe episode ends when the pole is more than 15 degrees from vertical, or the \\ncart moves more than 2.4 units from the center.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "''' \n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "A reward of +1 is provided for every timestep that the pole remains upright.\n",
    "The episode ends when the pole is more than 15 degrees from vertical, or the \n",
    "cart moves more than 2.4 units from the center.\n",
    "'''\n",
    "\n",
    "# env.reset()    #returns an initial observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 1,\n",
    "    'batch_size': 64,\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'num_channels': 512,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "class NNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(4, 128)\n",
    "        self.l2 = nn.Linear(128, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.l4 = nn.Linear(128, 32)\n",
    "        \n",
    "        self.dp = nn.Dropout(p = 0.2)\n",
    "        self.fc = nn.Linear(32, 2)    # want an action vector output: [log(prob right), log(prob left)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        #in_size = x.size(0)\n",
    "        x = F.relu(self.dp(self.l1(x)))\n",
    "        x = F.relu(self.dp(self.l2(x)))\n",
    "        x = F.relu(self.dp(self.l3(x)))\n",
    "        x = F.relu(self.dp(self.l4(x)))\n",
    "        \n",
    "        #x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(self.dp(x))\n",
    "        x = F.log_softmax(x, dim = -1)    # choose the dimension such that we get something like \n",
    "        return x                         # [exp(-0.6723) +  exp(-0.7144)] = 1 for the output   \n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        examples: list of examples, each example is of form (state, pi, v)\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "        for epoch in range(args.epochs):\n",
    "            print('EPOCH ::: ' + str(epoch+1))\n",
    "            self.model.train()     # set module in training mode\n",
    "            batch_idx = 0\n",
    "            \n",
    "            while batch_idx < int(len(examples)/args.batch_size):\n",
    "                \n",
    "                # --------------------- FORMAT DATA ----------------------\n",
    "                \n",
    "                # generate an array (of length batch_size) of random integers between 0 and len(examples)\n",
    "                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
    "                \n",
    "                # randomly rearrange selected examples and then transpose them\n",
    "                states, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
    "                print(\"Shapes: \", states.shape, pis.shape, vs.shape)\n",
    "                \n",
    "                #convert states, policies and state-values to a tensors of floats\n",
    "                states = torch.FloatTensor(np.array(states).astype(np.float64))\n",
    "                target_pis = torch.FloatTensor(np.array(pis))\n",
    "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "                # -------------------- PREDICT --------------------------\n",
    "                if args.cuda:  #if we're using the GPU:\n",
    "                    states, target_pis, target_vs = states.contiguous().cuda(), target_pis.contiguous().cuda(), target_vs.contiguous().cuda()\n",
    "                states, target_pis, target_vs = Variable(states), Variable(target_pis), Variable(target_vs)\n",
    "                \n",
    "\n",
    "                # -------------------- FEED FORWARD ----------------------        \n",
    "                out_pi, out_v = self.model(states)\n",
    "            \n",
    "                l_pi = self.loss_pi(target_pis, out_pi)\n",
    "                l_v = self.loss_v(target_vs, out_v)\n",
    "                total_loss = l_pi + l_v\n",
    "\n",
    "                # record loss\n",
    "                print(\"Pi loss: \", l_pi.data[0], states.size(0))\n",
    "                print(\"V loss: \", l_v.data[0], states.size(0))\n",
    "\n",
    "                # ----------- COMPUTE GRADS AND BACKPROP ----------------\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_idx += 1\n",
    "\n",
    "    def loss_pi(self, targets, outputs):\n",
    "        return -torch.sum(targets*outputs)/targets.size()[0]\n",
    "\n",
    "    def loss_v(self, targets, outputs):\n",
    "        return torch.sum((targets-outputs.view(-1))**2)/targets.size()[0]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def predict(self, state):\n",
    "        \"\"\"\n",
    "        state: np array with state\n",
    "        \"\"\"\n",
    "        # timing\n",
    "        start = time.time()\n",
    "\n",
    "        # preparing input\n",
    "        state = torch.FloatTensor(state.astype(np.float64))\n",
    "        if args.cuda: \n",
    "            state = state.contiguous().cuda()\n",
    "            \n",
    "        #A Variable wraps a Tensor. Variable also provides a backward method to perform backpropagation\n",
    "        state = torch.autograd.Variable(state, volatile=True)\n",
    "        state = state.view(1, self.state_x, self.state_y)\n",
    "\n",
    "        # sets mode to prediction\n",
    "        self.model.eval()\n",
    "        pi, v = self.model(state)\n",
    "\n",
    "        #print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n",
    "        return torch.exp(pi).data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n",
    "\n",
    "model = NNet()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "\n",
    "    def __init__(self, nnet):\n",
    "        self.nnet = nnet    #fuction handle\n",
    "        self.c_puct = 0.1\n",
    "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores #times edge s,a was visited\n",
    "        self.Ns = {}        # stores #times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "    def search(self, s, reward, done):\n",
    "        # ---------------- TERMINAL STATE ---------------\n",
    "        if done == True:\n",
    "            return reward\n",
    "\n",
    "        # ------------- EXPLORING FROM A LEAF NODE ----------------------\n",
    "        #check if the state has a policy from it yet, if not then its a leaf\n",
    "        if s not in self.Ps:\n",
    "            self.Ps[s], v = self.nnet.predict(s)\n",
    "            \n",
    "            #check if the neural net has assigned a +ve prob to any policy\n",
    "             \n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
    "            else:\n",
    "                # if they were all zero then they are equally probable: (this shouldn't usually happen)\n",
    "                # NB! All valid moves may = 0 if NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(\"All valid moves were masked, do workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "        \n",
    "\n",
    "        # ------------- GET BEST ACTION -----------------------------\n",
    "        # search through the valid actions and update the UCB for all actions then update best acions\n",
    "        max_u, best_a = -float(\"inf\"), -1\n",
    "        for a in range(1):\n",
    "            if (s,a) in self.Qsa:\n",
    "                u = self.Qsa[(s,a)] + self.cpuct*self.Ps[s][a]*np.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
    "            else:\n",
    "                u = self.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + 1e-8)     # Q = 0 ?\n",
    "            \n",
    "            if u > max_u:\n",
    "                max_u = u\n",
    "                best_a = a\n",
    "        a = best_a\n",
    "\n",
    "        \n",
    "        # ----------- RECURSION TO NEXT STATE ------------------------\n",
    "        sp, reward, done, info = env.step(a)\n",
    "        v = self.search(sp, reward, done)\n",
    "        \n",
    "\n",
    "        # ------------ BACKUP Q-VALUES AND N_VISITED -----------------\n",
    "        # after we reach the terminal condition then the stack unwinds and we\n",
    "        # propagate up the tree backing up Q and N as we go\n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
    "            self.Nsa[(s,a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v\n",
    "            self.Nsa[(s,a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration and Episode Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeEpisode(nnet):\n",
    "    examples = []\n",
    "    s, reward, done, info = env.reset()\n",
    "    #mcts = MCTS(nnet)  # initialise the MCTS\n",
    "    steps = 100\n",
    "    tot_reward = steps\n",
    "    \n",
    "    print(s)\n",
    "    for t in range(steps):\n",
    "        env.render()\n",
    "        #for _ in range(numMCTSSims):\n",
    "        #    mcts.search(s, reward, done)\n",
    "        \n",
    "        # choose a random action, 0 (left) or 1 (right) for now\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # observation/state s is the cart angle, or game image data from other envs\n",
    "        # bounded by [+-2.4, +-inf, +-12degrees, +-inf]\n",
    "        # which are the [position, velocity, angle, pole velocity at tip]\n",
    "        s, reward, done, info = env.step(action)\n",
    "        tot_reward -= reward\n",
    "                \n",
    "        #usually we add the rewards at the end of the episode\n",
    "        #usually append mcts.pi(s) too\n",
    "        examples.append([s, tot_reward])\n",
    "        \n",
    "        #a = random.choice(len(mcts.pi(s)), p = mcts.pi(s))\n",
    "        #s = game.nextState(s, a)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(examples)\n",
    "            return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023818285864886532\n",
      "Episode finished after 12 timesteps\n",
      "[[array([ 0.02312713, -0.22913655, -0.03721234,  0.24926613]), 99.0], [array([ 0.0185444 , -0.03350348, -0.03222702, -0.05491822]), 98.0], [array([ 0.01787433,  0.16206538, -0.03332538, -0.35759229]), 97.0], [array([ 0.02111564, -0.03256734, -0.04047723, -0.07560117]), 96.0], [array([ 0.02046429,  0.16311081, -0.04198925, -0.38077503]), 95.0], [array([ 0.02372651,  0.35880308, -0.04960475, -0.68639591]), 94.0], [array([ 0.03090257,  0.55457725, -0.06333267, -0.99427377]), 93.0], [array([ 0.04199411,  0.7504866 , -0.08321814, -1.30615599]), 92.0], [array([ 0.05700385,  0.94655904, -0.10934126, -1.62368476]), 91.0], [array([ 0.07593503,  1.14278499, -0.14181496, -1.94834827]), 90.0], [array([ 0.09879073,  1.3391032 , -0.18078192, -2.28142334]), 89.0], [array([ 0.12557279,  1.14606143, -0.22641039, -2.04944041]), 88.0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([ 0.02312713, -0.22913655, -0.03721234,  0.24926613]), 99.0],\n",
       " [array([ 0.0185444 , -0.03350348, -0.03222702, -0.05491822]), 98.0],\n",
       " [array([ 0.01787433,  0.16206538, -0.03332538, -0.35759229]), 97.0],\n",
       " [array([ 0.02111564, -0.03256734, -0.04047723, -0.07560117]), 96.0],\n",
       " [array([ 0.02046429,  0.16311081, -0.04198925, -0.38077503]), 95.0],\n",
       " [array([ 0.02372651,  0.35880308, -0.04960475, -0.68639591]), 94.0],\n",
       " [array([ 0.03090257,  0.55457725, -0.06333267, -0.99427377]), 93.0],\n",
       " [array([ 0.04199411,  0.7504866 , -0.08321814, -1.30615599]), 92.0],\n",
       " [array([ 0.05700385,  0.94655904, -0.10934126, -1.62368476]), 91.0],\n",
       " [array([ 0.07593503,  1.14278499, -0.14181496, -1.94834827]), 90.0],\n",
       " [array([ 0.09879073,  1.3391032 , -0.18078192, -2.28142334]), 89.0],\n",
       " [array([ 0.12557279,  1.14606143, -0.22641039, -2.04944041]), 88.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executeEpisode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyIterSP():\n",
    "    nnet.initNNet()\n",
    "    examples = []\n",
    "    \n",
    "    numIters, numEps = 10, 10\n",
    "    for i in range(numIters):\n",
    "        for e in range(numEps):\n",
    "            examples += executeEpisode(nnet)\n",
    "            \n",
    "        new_nnet = trainNNet(examples)\n",
    "        frac_win = pit(new_nnet, nnet)\n",
    "        \n",
    "        if frac_win > 0.55:\n",
    "            nnet = new_nnet\n",
    "            \n",
    "    return nnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
